% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpt_read.R
\name{parse_text}
\alias{parse_text}
\title{Parse documents to tokens for GPT}
\usage{
parse_text(
  file,
  remove_whitespace = TRUE,
  remove_special_chars = TRUE,
  remove_numbers = TRUE
)
}
\arguments{
\item{file}{A character string specifying the path to the input file.}

\item{remove_whitespace}{Logical indicating whether to remove leading and trailing whitespace from the text. Default is \code{TRUE}.}

\item{remove_special_chars}{Logical indicating whether to remove special characters from the text. Default is \code{TRUE}.}

\item{remove_numbers}{Logical indicating whether to remove numbers from the text. Default is \code{TRUE}.}
}
\value{
A list of character vectors, where each vector represents a chunk of tokens from the input document.
}
\description{
This function parses different types of documents (PDF, DOCX, TXT) and converts them into tokens for further processing with GPT (Generative Pre-trained Transformer) models. It removes whitespace, special characters, and numbers from the text, and splits it into chunks of tokens to avoid exceeding the model's token limit.
}
\examples{
\dontrun{
# Example 1: Parsing a PDF document
pdf_file <- "path/to/document.pdf"
parsed_tokens <- parse_text(file = pdf_file)

# Example 2: Parsing a DOCX document
docx_file <- "path/to/document.docx"
parsed_tokens <- parse_text(file = docx_file)

# Example 3: Parsing a TXT document
txt_file <- "path/to/document.txt"
parsed_tokens <- parse_text(file = txt_file)
}
}
\seealso{
\code{\link[pdftools]{pdf_text}}, \code{\link[readtext]{readtext}}, \code{\link{readLines}}
}
