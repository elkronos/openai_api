#' Table of Contents and Description
#' 
#' The code in this script is designed to allow the user to perform grid searches using the GPT API.
#' You can dynamically vary system messages (roles), temperature (randomness), and tokens (length)
#' so that you can dynamically compare results across parameters.
#' 
#' LINE 053 - call_api() - Call API
#' LINE 115 - gpt() - Query GPT
#' LINE 177 - gpt_search() - grid search GPT based on user specified variations
#' LINE 270 - import_rds_files() - Import .rds in working directory
#' 

# Load packages
library(httr)           # for RETRY(), add_headers(), content_type_json(), stop_for_status()
library(jsonlite)       # for toJSON()
library(stringr)        # for str_trim()
library(purrr)          # for possibly()
library(readr)          # for write_rds()

#' Call OpenAI's GPT API
#'
#' This function calls OpenAI's GPT API to generate a message using a given language model. 
#' It handles retries and error checking, and processes the response from the API for easier use.
#'
#' @param model Character. The identifier of the language model to use.
#' @param temperature Numeric. Controls randomness in the model's output. Range: 0.0 to 1.0.
#' @param max_tokens Integer. The maximum number of tokens to be generated by the model.
#' @param messages List. Messages to be passed to the model. Each message has two properties: 'role' and 'content'.
#' @param num_retries Integer. Number of retry attempts in case of a network error. Default: 3.
#' @param pause_base Numeric. The base of the exponential backoff for retries. Default: 1.
#'
#' @importFrom httr RETRY add_headers content_type_json stop_for_status
#' @importFrom jsonlite toJSON
#' @importFrom stringr str_trim
#'
#' @return A cleaned message generated by the model.
#'
#' @examples
#' \dontrun{
#'   # Define a list of messages to be sent to the model
#'   messages <- list(
#'     list('role' = 'system', 'content' = 'You are a helpful assistant.'),
#'     list('role' = 'user', 'content' = 'Who won the world series in 2020?')
#'   )
#'   
#'   # Call the function with appropriate parameters
#'   call_api('text-davinci-002', 0.5, 100, messages)
#' }
#'
#' @note This function requires a valid OpenAI API key, which should be stored in the `api_key` variable.
#'
#' @export
call_api <- function(model, temperature, max_tokens, messages, num_retries = 3, pause_base = 1) {
  response <- RETRY(
    "POST",
    url = "https://api.openai.com/v1/chat/completions", 
    add_headers(Authorization = paste("Bearer", api_key)),
    content_type_json(),
    encode = "json",
    times = num_retries,
    pause_base = pause_base,
    body = list(
      model = model,
      temperature = temperature,
      max_tokens = max_tokens,
      messages = messages
    )
  )
  
  # Check for HTTP errors
  stop_for_status(response)
  
  # Get and clean the message
  if (length(content(response)$choices) > 0) {
    message <- content(response)$choices[[1]]$message$content
  } else {
    message <- "The model did not return a message. You may need to increase max_tokens."
  }
  
  clean_message <- gsub("\n", " ", message) # replace newlines with spaces
  clean_message <- str_trim(clean_message) # trim white spaces
  return(clean_message)
}

#' GPT-based Text Generation
#'
#' This function uses a specified GPT model to generate a text based on a given prompt.
#' It can optionally include system messages in the conversation. It's built on top of the 'call_api' function.
#'
#' @param prompt Character. The user message to be processed by the language model.
#' @param model Character. The identifier of the language model to use. Default: "gpt-3.5-turbo".
#' @param temperature Numeric. Controls randomness in the model's output. Range: 0.0 to 1.0. Default: 0.5.
#' @param max_tokens Integer. The maximum number of tokens to be generated by the model. Default: 50.
#' @param system_messages List. Optional. System messages to include in the conversation. Default: NULL.
#'
#' @importFrom purrr append
#'
#' @return A message generated by the model based on the given prompt.
#'
#' @examples
#' \dontrun{
#'   # Use the GPT function with a user message
#'   gpt('Tell me a joke', model = 'gpt-3.5-turbo', temperature = 0.6, max_tokens = 40)
#'
#'   # Use the GPT function with a user message and a system message
#'   gpt('Tell me a joke', model = 'gpt-3.5-turbo', temperature = 0.6, max_tokens = 40,
#'       system_messages = 'You are a humoristic AI')
#' }
#'
#' @note This function requires a valid OpenAI API key, which should be stored in the `api_key` variable.
#'
#' @seealso \code{\link{call_api}}
#'
#' @export
gpt <- function(prompt, model = "gpt-3.5-turbo", temperature = 0.5, max_tokens = 50, system_messages = NULL) {
  # Start with the user's message
  messages <- list(list(role = "user", content = prompt))
  
  # If system messages were provided, prepend each one to the list
  if (!is.null(system_messages)) {
    for (system_message in system_messages) {
      messages <- append(list(list(role = "system", content = system_message)), messages)
    }
  }
  
  # Call the API
  result <- call_api(model, temperature, max_tokens, messages)
  
  return(result)
}

#' Grid Search Over GPT Models
#'
#' This function performs a grid search over the specified GPT models and parameters, using a given prompt.
#' Results can optionally be saved to disk in batches.
#'
#' @param prompt Character. The user message to be processed by the language model.
#' @param models Character vector. The identifiers of the language models to use. Default: c("gpt-3.5-turbo", "gpt-4").
#' @param temperatures Numeric vector. Controls randomness in the model's output. Default: seq(0.2, 1, by = 0.2).
#' @param max_tokens_range Integer vector of length 2. The range of maximum tokens to be generated by the models. Default: c(20, 100).
#' @param system_messages List of character vectors. System messages to include in the conversation. Default: list(NULL).
#' @param batch_size Integer. The number of results to accumulate before saving a batch to disk. Default: 100.
#' @param save_to_disk Logical. Whether to save results to disk. Default: FALSE.
#' @param file_path Character. The directory where batch files will be saved, if save_to_disk is TRUE. Default: "YOUR_PATH_HERE".
#'
#' @importFrom utils write.csv read.csv
#' @importFrom base rbind Sys.sleep
#' @importFrom purrr append
#' @importFrom readr write_rds read_rds
#'
#' @return A data frame of results with columns: model, temperature, max_tokens, system_message, and result.
#' If save_to_disk is TRUE, it saves the results in batches as .rds files and returns NULL.
#'
#' @examples
#' \dontrun{
#'   # Set up the system messages
#'   system_messages <- list("You are a funny comedian", "You are Dane Cook")
#'
#'   # Perform a grid search
#'   results <- gpt_search(
#'     prompt = "Tell me a joke",
#'     models = c("gpt-3.5-turbo", "gpt-4"),
#'     temperatures = c(0.2, 0.5, 0.8),
#'     max_tokens_range = c(20, 30),
#'     system_messages = system_messages,
#'     batch_size = 100,
#'     save_to_disk = TRUE,
#'     file_path = "your_path_here"
#'   )
#'}
#'
#' @note This function requires a valid OpenAI API key, which should be stored in the `api_key` variable.
#'
#' @seealso \code{\link{gpt}}
#'
#' @export
gpt_search <- function(prompt, models = c("gpt-3.5-turbo", "gpt-4"), temperatures = seq(0.2, 1, by = 0.2), 
                        max_tokens_range = c(20, 100), system_messages = list(NULL), batch_size = 100, 
                        save_to_disk = FALSE, file_path = "YOUR_PATH_HERE") {
  
  # Create a data frame with all combinations of parameters
  params <- expand.grid(models = models, 
                        temperatures = temperatures, 
                        max_tokens = seq(max_tokens_range[1], max_tokens_range[2]), 
                        system_messages = system_messages,
                        stringsAsFactors = FALSE)
  
  # Initialize results data frame
  results <- data.frame(model = character(), 
                        temperature = numeric(), 
                        max_tokens = integer(), 
                        system_message = character(), 
                        result = character(), 
                        stringsAsFactors = FALSE)
  
  # Initialize batch counter
  batch_counter <- 1
  
  # Iterate over each row of the parameters data frame
  for (i in seq_len(nrow(params))) {
    row <- params[i, ]
    
    # Call the gpt function and store the result
    result <- gpt(prompt = prompt, model = row$models, temperature = row$temperatures,
                  max_tokens = row$max_tokens, system_messages = row$system_messages)
    
    # Combine all system messages into a single string
    system_message_string <- paste(row$system_messages, collapse = "; ")
    
    # Create a new row with the result and the parameters
    new_row <- data.frame(model = row$models,  # Changed from row$model to row$models
                          temperature = row$temperatures,  # Changed from row$temperature to row$temperatures
                          max_tokens = row$max_tokens, 
                          system_message = system_message_string,  # Use the combined string
                          result = result,
                          stringsAsFactors = FALSE)
    
    # Add the new row to the results data frame
    results <- rbind(results, new_row)
    
    # Save to disk in batches if specified
    if (save_to_disk && nrow(results) >= batch_size) {
      write_rds(results, paste0(file_path, "/batch_", batch_counter, ".rds"))
      results <- data.frame(model = character(), 
                            temperature = numeric(), 
                            max_tokens = integer(), 
                            system_message = character(), 
                            result = character(), 
                            stringsAsFactors = FALSE) # Reset the results data frame
      batch_counter <- batch_counter + 1
    }
    
    # Rate limit handling
    Sys.sleep(3) # Adjust this value as needed to stay within the API's rate limit
  }
  
  # Save remaining results to disk if specified
  if (save_to_disk && nrow(results) > 0) {
    write_rds(results, paste0(file_path, "/batch_", batch_counter, ".rds"))
  }
  
  # Return the data frame of results if not saved to disk
  if (!save_to_disk) {
    return(results)
  }
}

#' Import RDS Files
#'
#' Set the working directory to the specified path and import RDS files.
#'
#' @param path The path to the directory containing the RDS files.
#'
#' @return A list of data frames containing the imported data from RDS files.
#'
#' @examples
#' \dontrun{
#' # Set working directory to your path
#' setwd("your_path_here")
#'
#' # Import one batch
#' readRDS("your_path/batch_1.rds") -> save
#'
#' # Import all RDS
#' dir_path <- "path_to_your_directory"
#' data_list <- import_rds_files(dir_path)
#' }
#'
#' @export
import_rds_files <- function(path) {
  # Set working directory to the specified path
  setwd(path)
  
  # Get a list of all RDS files in the directory
  rds_files <- list.files(path = path, pattern = "\\.rds$", full.names = TRUE)
  
  # Read each RDS file and store the results in a list
  data_list <- lapply(rds_files, readRDS)
  
  return(data_list)
}